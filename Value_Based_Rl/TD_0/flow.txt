TD(0) with V(s) Algorithm Flow

1. Initialization
   - Create FrozenLake-v1 environment.
   - Initialize Value Table V(s) with zeros (Size: Num States).
   - Set parameters: alpha, gamma, epsilon.

2. Training Loop (for each episode)
   a. Reset Environment to get initial State S.

   b. Episode Loop (while not Done)
      i.   Choose Action A
           - Get Lookahead Values using env.P (Model):
             For each action a':
               Value(a') = Sum(P(s'|s,a') * [R + gamma * V(s')])
           - Epsilon-Greedy:
             - Random (prob epsilon)
             - Max Value (prob 1-epsilon)

      ii.  Take Action A
           - Observe Reward R, Next State S'.

      iii. Update V(s) (TD Update)
           - Target = R + gamma * V(S')
           - Error = Target - V(S)
           - V(S) = V(S) + alpha * Error

      iv.  Transition
           - S <- S'

   c. Decay Epsilon

3. Output
   - Save trained V-table to 'v_table.pkl'.
