TD(0) - Temporal Difference Learning with Value Function V(s)

Explanation:
This folder implements the classic TD(0) algorithm for learning the State-Value Function V(s). Unlike Q-Learning which learns Action-Values Q(s,a), TD(0) focuses solely on how good a state is.

1. Value Function V(s):
   - We maintain a table of size equal to the number of states (16 for 4x4 FrozenLake).
   - V(s) represents the expected cumulative future reward from state s.

2. The Update Rule:
   - After observing a transition S -> R -> S', we update V(S):
     V(S) <- V(S) + alpha * [R + gamma * V(S') - V(S)]
   - This uses bootstrapping: estimating the value of S using the current estimate of V(S').

3. Control (Action Selection):
   - Since V(s) does not tell us which action is best directly, we use the Environment Model (Lookahead) to select actions.
   - We check all possible actions and calculate:
     Expected Value = Sum(prob * (reward + gamma * V(next_state)))
   - We choose the action with the highest Expected Value.
   - In a real-world scenario without a model, you would need Q-Learning or Sarsa. Since this is a known environment, we can demonstrate plain TD(0) with V(s) using 1-step lookahead.

4. Implementation details:
   - On-Policy / Off-Policy distinction is less relevant here as we are evaluating V for the greedy policy derived from itself (Generalized Policy Iteration).
